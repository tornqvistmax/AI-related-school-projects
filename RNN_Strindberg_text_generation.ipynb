{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RNN-Strindberg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yys7MyPDFoR"
      },
      "source": [
        "#Sammanfattning\n",
        "Detta projekt genomfördes i samband med en kurs på Umeå Universitet och ämnar att skapa en text-generator som med hjälp av data från August Strindbergs \"Inferno\" ska generera text i Strindberg-stil.\n",
        "Detta är mitt allra först RNN-projekt, med anledning av detta kommer koden till stor del följa den redan existerande exempel-koden som ges av Keras. Jag har själv försökt utveckla denna där jag kunnat och på ett övergripande sätt förklara det som pågår i koden. RNN står för Recurrent Neural Network och är en typ av nätverk som passar sig bra för just sekventiell data vilket är vad vi kommer hålla på med i den här koden. Utöver text så passar sig RNN bra för annan typ av sekventiell data, såsom t.ex börsdata och aktiekurser (olika typer av tidserier)m.m. I detta program kommer vi att använda en variant av RNN som kallas  LSTM (Long Short-term Memory). LSTM syftar till att lösa \"the vanishing gradient problem\" som är ett problem som ofta uppstår i RNNs där gradients blir försvinnande små vilket leder till mindre och mindre ändringar på viktningen av noderna i nätverket som i förlängningen leder till en inlärning som minskar och minskar. Denna LSTM-teknik gör sig väldigt bra inom NLP. LSTM har en förmåga att komma ihåg längre sekvenser jämfört med andra typer av RNN. Så i en längre mening där viktig information finns i början av meningen och där nätverket ska förutsäga slutet av meningen presterar LSTM generellt bättre.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM8l2xdTBxis"
      },
      "source": [
        "# Importerar bibliotek\n",
        "Importerar de olika biblioteken som skall komma att utnyttjas senare i programmet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2IqH-cF5lTD"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "#from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "#import io\n",
        "import requests\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_gGHgJTCILq"
      },
      "source": [
        "#Hämtar och förbereder datan\n",
        "Datan hämtas och bearbetas i olika numrerade steg som beskrivs nedan:\n",
        "\n",
        "1. Hämtar datan via länk.\n",
        "\n",
        "2. Ändrar samtliga bokstäver till gemener.\n",
        "\n",
        "3. Skriver ut den delen av texten som senare kommer att tas bort.\n",
        "\n",
        "4. Ignorerar inledningen av texten pga. irrelevans för det vi vill skapa.\n",
        "\n",
        "5. Skriver ut längden av den kvarvarande texten (med tecken som enhet).\n",
        "\n",
        "6. Hämtar och sparar samtliga unika tecken i en lista vid namn chars.\n",
        "\n",
        "7. Skriver ut antalet unika tecken med hjälp av längden på listan chars.\n",
        "\n",
        "8. För att det neurala nätverket ska kunna behandla vår data behöver vi först omvandla våra olika tecken till siffror. Därför gör vi två dictionaries:\n",
        "ett där varje tecken får en motsvarande siffra som kommer att representera tecknet genom det neurala nätverket. I detta första dictionary kommer tecknet att vara nyckeln och siffran blir därmed värdet. I det andra dictionariet gör vi precis tvärtom för att att lättare få åtkomst till datan efter den har behandlats av det neurala nätverket.\n",
        "\n",
        "9. Ställer in längden på sekvenserna(som blir nätverkets features) samt längden på steget mellan sekvenserna(vilket blir längden på nätverkets labels).\n",
        "\n",
        "10. Skapar en array för alla features och en array för alla labels. Delar sedan upp texten i features och labels med hjälp av en for-loop.\n",
        "\n",
        "11. Datan omformas till ett binärt vektor-format för att kunna hanteras av nätverket(One-hot encoding). Detta sker bl.a. genom användning av numpys zeros-funktion som returnerar en vektor med en form och datatyp som anges i argumenten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyW3QzqU52xE",
        "outputId": "21589bb8-dd5c-42ed-ee4c-455f70fcfa82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1.\n",
        "r = requests.get(\"http://www.gutenberg.org/cache/epub/29935/pg29935.txt\")\n",
        "\n",
        "#2.\n",
        "text = r.text.lower()\n",
        "\n",
        "#3.\n",
        "print(text[0:2575]);\n",
        "\n",
        "#4.\n",
        "text=text[2575:]\n",
        "\n",
        "#5.\n",
        "print('Length of text:', len(text), \"characters\")\n",
        "\n",
        "#6.\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "#7.\n",
        "print('total unique chars:', len(chars), \" characters\")\n",
        "\n",
        "#8.\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
        "num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "#9.\n",
        "seq_length = 40\n",
        "step = 3\n",
        "\n",
        "#10.\n",
        "features = []\n",
        "labels = []\n",
        "for i in range(0, len(text) - seq_length, step):\n",
        "    features.append(text[i: i + seq_length])\n",
        "    labels.append(text[i + seq_length])\n",
        "print('number of sequences:', len(features))\n",
        "print(\"length of one feature\", len(features[0]))\n",
        "print(\"length of one label\", len(labels[0]))\n",
        "\n",
        "#11.\n",
        "x = np.zeros((len(features), seq_length, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(features), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(features):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_to_num[char]] = 1\n",
        "    y[i, char_to_num[labels[i]]] = 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿the project gutenberg ebook of inferno, by august strindberg\r\n",
            "\r\n",
            "this ebook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  you may copy it, give it away or\r\n",
            "re-use it under the terms of the project gutenberg license included\r\n",
            "with this ebook or online at www.gutenberg.org\r\n",
            "\r\n",
            "\r\n",
            "title: inferno\r\n",
            "\r\n",
            "author: august strindberg\r\n",
            "\r\n",
            "release date: september 8, 2009 [ebook #29935]\r\n",
            "\r\n",
            "language: swedish\r\n",
            "\r\n",
            "\r\n",
            "*** start of this project gutenberg ebook inferno ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "produced by jens sadowski, and projekt runeberg for\r\n",
            "providing the scanned facsimiles.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "transcribers note:\r\n",
            "\r\n",
            "this e-text was produced from project runeberg's\r\n",
            "digital facsimile edition of\r\n",
            "  samlade verk #28: inferno och legender\r\n",
            "printed in 1914 and available at http://runeberg.org/strindbg/inferno/\r\n",
            "\r\n",
            "strindberg wrote this book originally in french. the swedish translation\r\n",
            "was done by eugène fahlstedt.\r\n",
            "\r\n",
            "this text has been edited so that this document only contains the book\r\n",
            "inferno.\r\n",
            "the table of content has been moved to the start of the book.\r\n",
            "\r\n",
            "text that was s p a c e d - o u t in the original text has been\r\n",
            "changed to use _italics_.\r\n",
            "\r\n",
            "project runeberg publishes free digital editions of nordic literature.\r\n",
            "learn more at =http://runeberg.org/=\r\n",
            "\r\n",
            "inferno\r\n",
            "\r\n",
            "av\r\n",
            "\r\n",
            "august strindberg\r\n",
            "\r\n",
            "stockholm\r\n",
            "albert bonniers förlag\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "_copyright. albert bonnier 1914._\r\n",
            "\r\n",
            "stockholm\r\n",
            "alb. bonniers boktryckeri 1914\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "innehåll.\r\n",
            "\r\n",
            "\r\n",
            "_inferno._\r\n",
            " 1. den osynliges hand .................................   7\r\n",
            " 2. ludvig den helige gör mig bekant med salig orfila ..  21\r\n",
            " 3. demonen frestar ....................................  27\r\n",
            " 4. det återvunna paradiset ............................  33\r\n",
            " 5. fallet och det förlorade paradiset .................  36\r\n",
            " 6. skärselden .........................................  42\r\n",
            " 7. helvetet ...........................................  93\r\n",
            " 8. beatrice ........................................... 122\r\n",
            " 9. swedenborg ......................................... 131\r\n",
            "10. utdrag ur en fördömds dagbok ....................... 149\r\n",
            "11. den evige har talat ................................ 163\r\n",
            "12. helvetet lössläppt ................................. 168\r\n",
            "13. pilgrimsfärd och försoning ......................... 178\r\n",
            "14. återlösaren ........................................ 184\r\n",
            "15. vedermödor ......................................... 190\r\n",
            "16. vartåt pekar vår väg? .............................. 197\r\n",
            "epilog ................................................. 203\r\n",
            "bibliografi ................................\n",
            "Length of text: 303577 characters\n",
            "total unique chars: 74  characters\n",
            "number of sequences: 101179\n",
            "length of one feature 40\n",
            "length of one label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by6dy88CCQ6W"
      },
      "source": [
        "# Skapar modellen\n",
        "\n",
        "Definerar modellen som sekventiell, Ger den två lager, Ett LSTM-lager för att hantera sekvenserna och ett Dense-lager för output(en output-neuron för varje unikt tecken). Dessa outputs omvandlas sedan till sannolikhetsvärden via softmax-funktionen. Båda dessa lager följer standard modellen som angivits i Google-guiderna. Utöver dessa lager så adderas en optimiserare(RMSprop) för att snabbare hitta det globala minimum där cost-funktionen har sitt lägsta möjliga värde. Även denna anges med någorlunda standardiserade parametrar där lr(learning rate) sattes till 0,01 (ju högre learning rate desto mer drastiska blir ändringarna på weights och biases). rho sätts till 0,9(också detta enligt Keras exemplet).\n",
        "\n",
        "Till sist så anges också modellens loss-funktion som är av typen categorical crossentropy vilket är det som passar bäst för denna typ utav klassifikationsproblem. Denna modell är nu redo att tränas på att givet en sekvens av bokstäver (representerade som siffror) kunna förutsäga kommande bokstäver(också dem representerade som siffror). På detta sätt kommer modellen senare givet en liten input(randomiserad) att på egen hand generera text!\n",
        "Det kommer att gå genom att modellen tränas på den träningsdata som tidigare matats in. Där features är informationen som modellen får för att kunna gör en kvalificerad gissning och labels är svaret som sedemera förväntas. Skillnaden mellan modellens gissning och det förväntade svaret är den data som modellen sedan kommer använda för att sedan förbättra sig själv(via backpropogation som kommer att förändra weights and biases), detta sker mella varje träningsepok. Backprogationen som nätverket använder sig av kallas för BPTT, vilket står för Backpropagation Through Time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-MaSdEggFQN"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(seq_length, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "optimizer = RMSprop(lr=0.01, rho=0.9)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQuLvSpDaFw6"
      },
      "source": [
        "#Hittar högsta sannolikheten\n",
        "\n",
        "Denna funktion tar den ursprungliga sannolikhetsfördelning och viktar om den för att senare returnera ett teckenindex. Detta sker genom att funktionen tar emot den tidigare sannolikhetsfördelningen av de olika tecknen samt ett temperatur-värde. För temperaturer så är det så att ett högt värde leder till att skillnaderna mellan de olika sannolikheterna inte blir så stora medan ett lägre värde gör att nätverket blir mer \"säker på sin sak\" och att skillnaderna på sannolikheterna växer. Dvs. att temperaturen är det som styr omviktandet av vår sannolikhetsfördelning. Funktionen säkerställer sedan att när alla dessa sannolikheter adderas så blir dem tillsammans 1.0. Därefter så tar funktionen och returnerar det högsta värdet för alla uppskattade sannolikheter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YYNHDWjIDAy"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4suO8A8FIEL-"
      },
      "source": [
        "#Genererar texten\n",
        "\n",
        "Denna funktion aktiveras efter varje epok och tar alltså emot den senaste epoken som argument och genererar sedemera texten. Först skriver funktionen ut vilken epok som den för tillfället använder sig av. Sen skapar funktionen ett seed som behövs för att senare trigga igång genereringen. Detta seed skapas genom en randomiserad siffra som tas fram av den inbyggda randint-funktionen. Denna funktion ges två argument: lägsta möjliga siffra och högsta möjliga siffra. Den randomiserade siffran ska senare användas för att ge nätverket en pseudorandomiserad sekvens(seed). \n",
        "Vi tar och beskriver funktionen stegvis:\n",
        "\n",
        "1. Skriver ut vilken epok som nätverk ligger på.\n",
        "\n",
        "2. Tar fram randomiserad siffra.\n",
        "\n",
        "3. Anger en loop med varierande temperaturer. Samt skriver ut aktuell temperatur för varje iteration.\n",
        "\n",
        "4. Skapar tom sträng för genererad text. Hämtar en sekvens(seed) från texten utifrån den genererade siffran. Denne sekvens läggs sedan till i den strängen för genererad text. Skriver ut aktuellt seed. \n",
        "\n",
        "5. Skapar en for-loop med 400 iterationer. Där varje iteration genererar en ny bokstav.\n",
        "\n",
        "6. Vi konverterar tecken till index.\n",
        "\n",
        "7. Genererar sannolikhetsfördelningen.\n",
        "\n",
        "8. Skickar in sannolikhetsfördelningen samt aktuell temperatur till sample-funktionen som i sin tur returnerar indexet för det tecken med högst sannolikhet.\n",
        "\n",
        "9. Konverterar det returnerade indexet till ett tecken med hjälp av det tidigare skapade dictionariet.\n",
        "\n",
        "10. Tar bort ett tecken från vår sekvens(seed) och lägger till det nya tecknet.\n",
        "\n",
        "11. Skriver ut det nya tecknet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVmpG5aDaDoh"
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    #1.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    #2.\n",
        "    start_index = random.randint(0, len(text) - seq_length - 1)\n",
        "\n",
        "    #3.\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        #4.\n",
        "        generated_text = ''\n",
        "        sentence = text[start_index: start_index + seq_length]\n",
        "        generated_text += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated_text)\n",
        "        #5.\n",
        "        for i in range(400):\n",
        "\n",
        "            #6.\n",
        "            x_pred = np.zeros((1, seq_length, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_to_num[char]] = 1.\n",
        "\n",
        "            #7.\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            \n",
        "            #8. \n",
        "            next_index = sample(preds, diversity)\n",
        "\n",
        "            #9.\n",
        "            next_char = num_to_char[next_index]\n",
        "\n",
        "            #10.\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            11.\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC-LWwOKIInn"
      },
      "source": [
        "#Träningen\n",
        "\n",
        "Här sätter vi igång träningen via model.fit som i sin tur anropar print_callback som med hjälp av ett LambadCallback talar om för programmet att anropa funktionen on_epoch_end vid varje epok-slut. Utöver detta så anger vi också batch_size och antalet epoker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rteuAGzYII76",
        "outputId": "86298d3b-cdfc-454d-fdbd-13342afb9f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=20,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "101179/101179 [==============================] - 72s 714us/step - loss: 2.2601\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"g form\n",
            "hopknäpptes till en bönfallande \"\n",
            "mig mig min som min fråg mig min som mig mig mig mig mig mig med mig som han min tocker mig mig som mig mig en mig för den för mig mig mig min tocker mig mig mig min för min till min förstra sin tockan som mig för det det min var mig för den som min för strada för det mig min förstrade och som min för mig för det min till min till min som min för mig min to                                         \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"g form\n",
            "hopknäpptes till en bönfallande \"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "order för starer av till med kor mig för det nänge sin som för en sinna det och lärda som frår har sina som med mig till min\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"g form\n",
            "hopknäpptes till en bönfallande \"\n",
            "\n",
            "\n",
            "\n",
            "bödjag sesför\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"g form\n",
            "hopknäpptes till en bönfallande \"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "patu troå\n",
            "Epoch 2/20\n",
            " 43520/101179 [===========>..................] - ETA: 38s - loss: 1.9410"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c09c556674af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=[print_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}